{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64ce7843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tool:\n",
    "# Deep learning：keras\n",
    "# Traditional machine learning：sklearn\n",
    "\n",
    "# The model involved in the training are as follows：\n",
    "# CNN, Wordvec+CNN\n",
    "# LSTM, word2vec+LSTM\n",
    "# KNN\n",
    "# SVM\n",
    "# naive bayes\n",
    "# Logistic Regression\n",
    "# Random forest classifier\n",
    "# Decition tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd71f0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras \n",
    "# !pip install tensorflow==2.2\n",
    "# !pip install numpy\n",
    "# !pip install gensim\n",
    "\n",
    "# print(np.__version__)\n",
    "# print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be7fc9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(train_texts,train_labels,test_texts,test_labels):\n",
    "    #coding:utf-8\n",
    "    print ('*load texts:')\n",
    "    train_texts = open(train_texts,encoding='UTF-8').read().split('\\n')\n",
    "    train_labels = open(train_labels,encoding='UTF-8').read().split('\\n')\n",
    "    test_texts = open(test_texts,encoding='UTF-8').read().split('\\n')\n",
    "    test_labels = open(test_labels,encoding='UTF-8').read().split('\\n')\n",
    "    all_texts = train_texts + test_texts\n",
    "    all_labels = train_labels + test_labels\n",
    "    return all_texts,all_labels,train_texts,train_labels,test_texts,test_labels\n",
    "    \n",
    "def creat_tokenizer(all_texts):\n",
    "    print ('*tokenizer:')\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    from tensorflow.keras.utils import to_categorical\n",
    "    import numpy as np\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(all_texts)\n",
    "    sequences = tokenizer.texts_to_sequences(all_texts)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    labels = to_categorical(np.asarray(all_labels))\n",
    "    print('The shape of data tensor:', data.shape)\n",
    "    print('The shape of label tensor:', labels.shape)\n",
    "    return data,labels,word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2be6e75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*load texts:\n",
      "*tokenizer:\n",
      "Found 205393 unique tokens.\n",
      "The shape of data tensor: (14104, 100)\n",
      "The shape of label tensor: (14104, 14)\n",
      "*split data set:\n",
      "train docs: 9167\n",
      "val docs: 2116\n",
      "test docs: 2821\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras\n",
    "import keras\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "EMBEDDING_DIM = 400\n",
    "VALIDATION_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.2\n",
    "\n",
    "all_texts,all_labels,train_texts,train_labels,test_texts,test_labels=load_text('text_stopwords.txt','categroy.txt','new_text_stopwords.txt','ner_cate.txt')\n",
    "\n",
    "data,labels,word_index=creat_tokenizer(all_texts)\n",
    "\n",
    "print ('*split data set:')\n",
    "# split the data into training set, validation set, and test set\n",
    "p1 = int(len(data)*(1-VALIDATION_SPLIT-TEST_SPLIT))\n",
    "p2 = int(len(data)*(1-TEST_SPLIT))\n",
    "x_train = data[:p1]\n",
    "y_train = labels[:p1]\n",
    "x_val = data[p1:p2]\n",
    "y_val = labels[p1:p2]\n",
    "x_test = data[p2:]\n",
    "y_test = labels[p2:]\n",
    "print ('train docs: '+str(len(x_train)))\n",
    "print ('val docs: '+str(len(x_val)))\n",
    "print ('test docs: '+str(len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e34b4978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_define(embedding_layer,embedding_dim,max_sequence_length):\n",
    "    #define for CNN\n",
    "    print ('*define model CNN:')\n",
    "    from keras.layers import Dense, Input, Flatten, Dropout\n",
    "    from keras.layers import Conv1D, MaxPooling1D, Embedding, GlobalMaxPooling1D\n",
    "    from keras.models import Sequential\n",
    "#     from keras.utils import plot_model\n",
    "\n",
    "    model = Sequential()\n",
    "    if embedding_layer==None:  \n",
    "        model.add(Embedding(len(word_index) + 1, embedding_dim, input_length=max_sequence_length))\n",
    "    else:\n",
    "        model.add(embedding_layer)\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(250, 3, padding='valid', activation='relu', strides=1))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(embedding_dim, activation='relu'))\n",
    "    model.add(Dense(labels.shape[1], activation='softmax'))\n",
    "    model.summary()\n",
    "#     plot_model(model, to_file='model.png',show_shapes=True)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
    "    print (model.metrics_names)\n",
    "    return model\n",
    "\n",
    "def LSTM_define(embedding_layer,embedding_dim,max_sequence_length):\n",
    "    #trainning for lstm\n",
    "    print ('*define model lstm：')\n",
    "    from keras.layers import Dense, Input, Flatten, Dropout\n",
    "    from keras.layers import LSTM, Embedding\n",
    "    from keras.models import Sequential\n",
    "\n",
    "    model = Sequential()\n",
    "    if embedding_layer==None:\n",
    "        model.add(Embedding(len(word_index) + 1, embedding_dim, input_length=max_sequence_length))\n",
    "    else:\n",
    "        model.add(embedding_layer)\n",
    "    model.add(LSTM(200, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(labels.shape[1], activation='softmax'))\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
    "    print (model.metrics_names)\n",
    "    return model\n",
    "    \n",
    "def model_train(model_name,model):\n",
    "    model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=2, batch_size=128)\n",
    "    model.save(model_name)\n",
    "\n",
    "    print ('*testing model:')\n",
    "    print (model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e0fe03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*define model CNN:\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 400)          82157600  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 400)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 98, 250)           300250    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 32, 250)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 400)               3200400   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 14)                5614      \n",
      "=================================================================\n",
      "Total params: 85,663,864\n",
      "Trainable params: 85,663,864\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "['loss', 'acc']\n",
      "Train on 9167 samples, validate on 2116 samples\n",
      "Epoch 1/2\n",
      "9167/9167 [==============================] - 50s 6ms/step - loss: 1.9257 - acc: 0.3635 - val_loss: 1.3926 - val_acc: 0.4197\n",
      "Epoch 2/2\n",
      "9167/9167 [==============================] - 50s 5ms/step - loss: 0.7611 - acc: 0.7652 - val_loss: 0.8687 - val_acc: 0.7434\n",
      "*testing model:\n",
      "2821/2821 [==============================] - 1s 245us/step\n",
      "[0.8247698631239292, 0.7777383923530579]\n"
     ]
    }
   ],
   "source": [
    "model=CNN_define(None,EMBEDDING_DIM,MAX_SEQUENCE_LENGTH)\n",
    "model_train('CNN.h5',model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f15f7fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*define model lstm：\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 100, 400)          82157600  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               480800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 14)                2814      \n",
      "=================================================================\n",
      "Total params: 82,641,214\n",
      "Trainable params: 82,641,214\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "['loss', 'acc']\n",
      "Train on 9167 samples, validate on 2116 samples\n",
      "Epoch 1/2\n",
      "9167/9167 [==============================] - 66s 7ms/step - loss: 2.2471 - acc: 0.2541 - val_loss: 2.1992 - val_acc: 0.2141\n",
      "Epoch 2/2\n",
      "9167/9167 [==============================] - 64s 7ms/step - loss: 1.4820 - acc: 0.5371 - val_loss: 1.7668 - val_acc: 0.4003\n",
      "*testing model:\n",
      "2821/2821 [==============================] - 2s 802us/step\n",
      "[1.535922124258215, 0.5108117461204529]\n"
     ]
    }
   ],
   "source": [
    "model=LSTM_define(None,EMBEDDING_DIM,MAX_SEQUENCE_LENGTH)\n",
    "model_train('LSTM.h5',model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ade9c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4) load word2vec as embedding...\n",
      "59395 words not in w2v model\n"
     ]
    }
   ],
   "source": [
    "print ('(4) load word2vec as embedding...')\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('wiki.zh.text.vector', binary=False)\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "not_in_model = 0\n",
    "in_model = 0\n",
    "for word, i in word_index.items(): \n",
    "    if str(word) in w2v_model:\n",
    "        in_model += 1\n",
    "        embedding_matrix[i] = np.asarray(w2v_model[str(word)], dtype='float32')\n",
    "    else:\n",
    "        not_in_model += 1\n",
    "print (str(not_in_model)+' words not in w2v model')\n",
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(len(word_index) + 1,EMBEDDING_DIM,weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62402289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*define model CNN:\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 100, 400)          82157600  \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100, 400)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 98, 250)           300250    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 32, 250)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 400)               3200400   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 14)                5614      \n",
      "=================================================================\n",
      "Total params: 85,663,864\n",
      "Trainable params: 3,506,264\n",
      "Non-trainable params: 82,157,600\n",
      "_________________________________________________________________\n",
      "['loss', 'acc']\n",
      "Train on 9167 samples, validate on 2116 samples\n",
      "Epoch 1/2\n",
      "9167/9167 [==============================] - 8s 832us/step - loss: 4.3913 - acc: 0.4715 - val_loss: 1.5196 - val_acc: 0.5964\n",
      "Epoch 2/2\n",
      "9167/9167 [==============================] - 8s 818us/step - loss: 0.8241 - acc: 0.7595 - val_loss: 1.0930 - val_acc: 0.7146\n",
      "*testing model:\n",
      "2821/2821 [==============================] - 1s 230us/step\n",
      "[0.9982556465541754, 0.7320099472999573]\n"
     ]
    }
   ],
   "source": [
    "model=CNN_define(embedding_layer,EMBEDDING_DIM,MAX_SEQUENCE_LENGTH)\n",
    "model_train('CNN_word2vec.h5',model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b53baf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*define model lstm：\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 100, 400)          82157600  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 200)               480800    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 14)                2814      \n",
      "=================================================================\n",
      "Total params: 82,641,214\n",
      "Trainable params: 483,614\n",
      "Non-trainable params: 82,157,600\n",
      "_________________________________________________________________\n",
      "['loss', 'acc']\n",
      "Train on 9167 samples, validate on 2116 samples\n",
      "Epoch 1/2\n",
      "9167/9167 [==============================] - 16s 2ms/step - loss: 1.4852 - acc: 0.5398 - val_loss: 0.9798 - val_acc: 0.7136\n",
      "Epoch 2/2\n",
      "9167/9167 [==============================] - 17s 2ms/step - loss: 0.9405 - acc: 0.7074 - val_loss: 0.7702 - val_acc: 0.7462\n",
      "*testing model:\n",
      "2821/2821 [==============================] - 2s 735us/step\n",
      "[0.8895783752931935, 0.7465437650680542]\n"
     ]
    }
   ],
   "source": [
    "model=LSTM_define(embedding_layer,EMBEDDING_DIM,MAX_SEQUENCE_LENGTH)\n",
    "model_train('LSTM_word2vec.h5',model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b53e0b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfid(all_texts):\n",
    "    print ('*tfidtransfor:')\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer   \n",
    "    count_v0= CountVectorizer();  \n",
    "    counts_all = count_v0.fit_transform(all_texts);\n",
    "    count_v1= CountVectorizer(vocabulary=count_v0.vocabulary_);  \n",
    "    counts_train = count_v1.fit_transform(train_texts);   \n",
    "    print (\"the shape of train is \"+repr(counts_train.shape))  \n",
    "    count_v2 = CountVectorizer(vocabulary=count_v0.vocabulary_);  \n",
    "    counts_test = count_v2.fit_transform(test_texts);  \n",
    "    print (\"the shape of test is \"+repr(counts_test.shape))\n",
    "\n",
    "    tfidftransformer = TfidfTransformer();    \n",
    "    train_data = tfidftransformer.fit(counts_train).transform(counts_train);\n",
    "    test_data = tfidftransformer.fit(counts_test).transform(counts_test); \n",
    "\n",
    "    x_train = train_data\n",
    "    y_train = train_labels\n",
    "    x_test = test_data\n",
    "    y_test = test_labels\n",
    "    return x_train,y_train,x_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6849f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*tfidtransfor:\n",
      "the shape of train is (5590, 200364)\n",
      "the shape of test is (8514, 200364)\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train,x_test,y_test=tfid(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bea3acb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model,x_test,y_test):\n",
    "    preds=model.predict(x_test)\n",
    "    num=0\n",
    "    preds=preds.tolist()\n",
    "    for i,pred in enumerate(preds):\n",
    "        if int(pred) == int(y_test[i]):\n",
    "            num += 1\n",
    "    return num, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e59a858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*KNN:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 1, precision_score:0.8202959830866807\n",
      "f1_score:0.7174386211598571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 2, precision_score:0.7378435517970402\n",
      "f1_score:0.6404781546359531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 3, precision_score:0.7807141179234203\n",
      "f1_score:0.6876219828738407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 4, precision_score:0.7788348602302091\n",
      "f1_score:0.6840624285501783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 5, precision_score:0.7885835095137421\n",
      "f1_score:0.698792451711796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 6, precision_score:0.785882076579751\n",
      "f1_score:0.6965410305564748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 7, precision_score:0.7926943857176415\n",
      "f1_score:0.704232499412351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 8, precision_score:0.7932816537467701\n",
      "f1_score:0.7025800806454455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 9, precision_score:0.7964529011040639\n",
      "f1_score:0.7082660223448582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 10, precision_score:0.7962179938924125\n",
      "f1_score:0.707351958213011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 11, precision_score:0.7963354474982381\n",
      "f1_score:0.7049330720342548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 12, precision_score:0.7935165609584214\n",
      "f1_score:0.7044715414057753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 13, precision_score:0.7939863753817242\n",
      "f1_score:0.7045985503237356\n",
      "K= 14, precision_score:0.7937514681700728\n",
      "f1_score:0.7054861869935919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "print ('*KNN:')\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_pred=[0,1,1,1,2,2]\n",
    "y_true=[0,1,0,2,1,1]\n",
    "\n",
    "for x in range(1,15):  \n",
    "    knnclf = KNeighborsClassifier(n_neighbors=x)\n",
    "    knnclf.fit(x_train,y_train)  \n",
    "    num, preds=test_model(knnclf,x_test,y_test)\n",
    "    print ('K= '+str(x)+', precision_score:' + str(float(num) / len(preds)))\n",
    "    print ('f1_score:' + str(f1_score(y_test,preds,average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9bba25e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*SVM：\n",
      "precision_score:0.9007517030772845\n",
      "f1_score:0.8149666268208976\n"
     ]
    }
   ],
   "source": [
    "print ('*SVM：')\n",
    "from sklearn.svm import SVC   \n",
    "svclf = SVC(kernel = 'linear') \n",
    "svclf.fit(x_train,y_train)  \n",
    "num, preds=test_model(svclf,x_test,y_test)\n",
    "print ('precision_score:' + str(float(num) / len(preds)))\n",
    "print ('f1_score:' + str(f1_score(y_test,preds,average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d75c41e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Naive Bayes：\n",
      "precision_score:0.9021611463471929\n",
      "f1_score:0.8212736580324363\n"
     ]
    }
   ],
   "source": [
    "print ('*Naive Bayes：')\n",
    "from sklearn.naive_bayes import MultinomialNB  \n",
    "\n",
    "clf = MultinomialNB(alpha = 0.01)   \n",
    "clf.fit(x_train, y_train)\n",
    "num, preds=test_model(clf,x_test,y_test)\n",
    "print ('precision_score:' + str(float(num) / len(preds)))\n",
    "print ('f1_score:' + str(f1_score(y_test,preds,average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8cf3cb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Logistic Regression：\n",
      "precision_score:0.9003993422598073\n",
      "f1_score:0.8039468213952466\n"
     ]
    }
   ],
   "source": [
    "print ('*Logistic Regression：')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log =  LogisticRegression(C=1e10,max_iter=1000) \n",
    "log.fit(x_train, y_train)\n",
    "num, preds=test_model(log,x_test,y_test)\n",
    "print ('precision_score:' + str(float(num) / len(preds)))\n",
    "print ('f1_score:' + str(f1_score(y_test,preds,average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7203a4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Random Forest Classifier：\n",
      "precision_score:0.8803147756636128\n",
      "f1_score:0.814149079603147\n"
     ]
    }
   ],
   "source": [
    "print ('*Random Forest Classifier：')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc =  RandomForestClassifier(n_estimators=500,  max_features='sqrt',random_state=10)\n",
    "rfc.fit(x_train, y_train)\n",
    "num, preds=test_model(rfc,x_test,y_test)\n",
    "print ('precision_score:' + str(float(num) / len(preds)))\n",
    "print ('f1_score:' + str(f1_score(y_test,preds,average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3dda5080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Decision Tree Classifier：\n",
      "precision_score:0.7668545924359877\n",
      "f1_score:0.6721781847170548\n"
     ]
    }
   ],
   "source": [
    "print ('*Decision Tree Classifier：')\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtc =  DecisionTreeClassifier(random_state=77)\n",
    "dtc.fit(x_train, y_train)\n",
    "num, preds=test_model(dtc,x_test,y_test)\n",
    "print ('precision_score:' + str(float(num) / len(preds)))\n",
    "print ('f1_score:' + str(f1_score(y_test,preds,average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed554fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
