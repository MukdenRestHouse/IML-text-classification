{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64ce7843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 工具\n",
    "# 深度学习：keras\n",
    "# 传统机器学习：sklearn\n",
    "# 参与比较的机器学习方法\n",
    "\n",
    "# CNN \n",
    "# LSTM \n",
    "# 朴素贝叶斯\n",
    "# KNN\n",
    "# SVM\n",
    "# Logisticre Gression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd71f0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.2\n",
    "# !pip install numpy\n",
    "# !pip install gensim\n",
    "\n",
    "# print(np.__version__)\n",
    "# print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be7fc9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(train_texts,train_labels,test_texts,test_labels):\n",
    "    #coding:utf-8\n",
    "    print ('*load texts:')\n",
    "    train_texts = open(train_texts,encoding='UTF-8').read().split('\\n')\n",
    "    train_labels = open(train_labels,encoding='UTF-8').read().split('\\n')\n",
    "    test_texts = open(test_texts,encoding='UTF-8').read().split('\\n')\n",
    "    test_labels = open(test_labels,encoding='UTF-8').read().split('\\n')\n",
    "    all_texts = train_texts + test_texts\n",
    "    all_labels = train_labels + test_labels\n",
    "    return all_texts,all_labels,train_texts,train_labels,test_texts,test_labels\n",
    "    \n",
    "def creat_tokenizer(all_texts):\n",
    "    print ('*tokenizer:')\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    from tensorflow.keras.utils import to_categorical\n",
    "    import numpy as np\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(all_texts)\n",
    "    sequences = tokenizer.texts_to_sequences(all_texts)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    labels = to_categorical(np.asarray(all_labels))\n",
    "    print('Shape of data tensor:', data.shape)\n",
    "    print('Shape of label tensor:', labels.shape)\n",
    "    return data,labels,word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2be6e75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*load texts:\n",
      "*tokenizer:\n",
      "Found 206233 unique tokens.\n",
      "Shape of data tensor: (14104, 100)\n",
      "Shape of label tensor: (14104, 14)\n",
      "*split data set:\n",
      "train docs: 9026\n",
      "val docs: 2257\n",
      "test docs: 2821\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras\n",
    "import keras\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "EMBEDDING_DIM = 400\n",
    "VALIDATION_SPLIT = 0.16\n",
    "TEST_SPLIT = 0.2\n",
    "\n",
    "all_texts,all_labels,train_texts,train_labels,test_texts,test_labels=load_text('text.txt','categroy.txt','new_text.txt','ner_cate.txt')\n",
    "\n",
    "data,labels,word_index=creat_tokenizer(all_texts)\n",
    "\n",
    "print ('*split data set:')\n",
    "# split the data into training set, validation set, and test set\n",
    "p1 = int(len(data)*(1-VALIDATION_SPLIT-TEST_SPLIT))\n",
    "p2 = int(len(data)*(1-TEST_SPLIT))\n",
    "x_train = data[:p1]\n",
    "y_train = labels[:p1]\n",
    "x_val = data[p1:p2]\n",
    "y_val = labels[p1:p2]\n",
    "x_test = data[p2:]\n",
    "y_test = labels[p2:]\n",
    "print ('train docs: '+str(len(x_train)))\n",
    "print ('val docs: '+str(len(x_val)))\n",
    "print ('test docs: '+str(len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e34b4978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_define(embedding_layer,embedding_dim,max_sequence_length):\n",
    "    #define for CNN\n",
    "    print ('*define model CNN:')\n",
    "    from keras.layers import Dense, Input, Flatten, Dropout\n",
    "    from keras.layers import Conv1D, MaxPooling1D, Embedding, GlobalMaxPooling1D\n",
    "    from keras.models import Sequential\n",
    "    from keras.utils import plot_model\n",
    "\n",
    "    model = Sequential()\n",
    "    if embedding_layer==None:  \n",
    "        model.add(Embedding(len(word_index) + 1, embedding_dim, input_length=max_sequence_length))\n",
    "    else:\n",
    "        model.add(embedding_layer)\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(250, 3, padding='valid', activation='relu', strides=1))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(embedding_dim, activation='relu'))\n",
    "    model.add(Dense(labels.shape[1], activation='softmax'))\n",
    "    model.summary()\n",
    "    # plot_model(model, to_file='model.png',show_shapes=True)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
    "    print (model.metrics_names)\n",
    "    return model\n",
    "\n",
    "def LSTM_define(embedding_layer,embedding_dim,max_sequence_length):\n",
    "    #trainning for lstm\n",
    "    print ('*define model lstm：')\n",
    "    from keras.layers import Dense, Input, Flatten, Dropout\n",
    "    from keras.layers import LSTM, Embedding\n",
    "    from keras.models import Sequential\n",
    "\n",
    "    model = Sequential()\n",
    "    if embedding_layer==None:\n",
    "        model.add(Embedding(len(word_index) + 1, embedding_dim, input_length=max_sequence_length))\n",
    "    else:\n",
    "        model.add(embedding_layer)\n",
    "    model.add(LSTM(200, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(labels.shape[1], activation='softmax'))\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
    "    print (model.metrics_names)\n",
    "    return model\n",
    "    \n",
    "def model_train(model_name,model):\n",
    "    model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=2, batch_size=128)\n",
    "    model.save(model_name)\n",
    "\n",
    "    print ('*testing model:')\n",
    "    print (model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e0fe03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*define model CNN:\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 100, 400)          82493600  \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100, 400)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 98, 250)           300250    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 32, 250)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 400)               3200400   \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 14)                5614      \n",
      "=================================================================\n",
      "Total params: 85,999,864\n",
      "Trainable params: 85,999,864\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "['loss', 'acc']\n",
      "Train on 9026 samples, validate on 2257 samples\n",
      "Epoch 1/2\n",
      "9026/9026 [==============================] - 51s 6ms/step - loss: 2.1368 - acc: 0.2905 - val_loss: 1.3169 - val_acc: 0.6735\n",
      "Epoch 2/2\n",
      "9026/9026 [==============================] - 50s 5ms/step - loss: 1.0714 - acc: 0.6526 - val_loss: 1.6058 - val_acc: 0.4936\n",
      "*testing model:\n",
      "2821/2821 [==============================] - 1s 222us/step\n",
      "[1.4706733739363391, 0.5473236441612244]\n"
     ]
    }
   ],
   "source": [
    "model=CNN_define(None,EMBEDDING_DIM,MAX_SEQUENCE_LENGTH)\n",
    "model_train('CNN.h5',model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f15f7fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*define model lstm：\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 100, 400)          82493600  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               480800    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 14)                2814      \n",
      "=================================================================\n",
      "Total params: 82,977,214\n",
      "Trainable params: 82,977,214\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "['loss', 'acc']\n",
      "Train on 9026 samples, validate on 2257 samples\n",
      "Epoch 1/2\n",
      "9026/9026 [==============================] - 67s 7ms/step - loss: 2.2828 - acc: 0.2314 - val_loss: 2.2457 - val_acc: 0.1573\n",
      "Epoch 2/2\n",
      "9026/9026 [==============================] - 65s 7ms/step - loss: 1.6984 - acc: 0.4558 - val_loss: 1.9405 - val_acc: 0.3389\n",
      "*testing model:\n",
      "2821/2821 [==============================] - 2s 844us/step\n",
      "[1.6512394015225313, 0.49202409386634827]\n"
     ]
    }
   ],
   "source": [
    "model=LSTM_define(None,EMBEDDING_DIM,MAX_SEQUENCE_LENGTH)\n",
    "model_train('LSTM.h5',model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "920176f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4) load word2vec as embedding...\n",
      "59406 words not in w2v model\n"
     ]
    }
   ],
   "source": [
    "print ('(4) load word2vec as embedding...')\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from keras.utils import plot_model\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('wiki.zh.text.vector', binary=False)\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "not_in_model = 0\n",
    "in_model = 0\n",
    "for word, i in word_index.items(): \n",
    "    if str(word) in w2v_model:\n",
    "        in_model += 1\n",
    "        embedding_matrix[i] = np.asarray(w2v_model[str(word)], dtype='float32')\n",
    "    else:\n",
    "        not_in_model += 1\n",
    "print (str(not_in_model)+' words not in w2v model')\n",
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(len(word_index) + 1,EMBEDDING_DIM,weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a23917e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*define model CNN:\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 100, 400)          82493600  \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 100, 400)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 98, 250)           300250    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 32, 250)           0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 400)               3200400   \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 14)                5614      \n",
      "=================================================================\n",
      "Total params: 85,999,864\n",
      "Trainable params: 3,506,264\n",
      "Non-trainable params: 82,493,600\n",
      "_________________________________________________________________\n",
      "['loss', 'acc']\n",
      "Train on 9026 samples, validate on 2257 samples\n",
      "Epoch 1/2\n",
      "9026/9026 [==============================] - 7s 765us/step - loss: 4.2814 - acc: 0.3637 - val_loss: 1.3118 - val_acc: 0.6292\n",
      "Epoch 2/2\n",
      "9026/9026 [==============================] - 7s 769us/step - loss: 1.0312 - acc: 0.6969 - val_loss: 0.8892 - val_acc: 0.7621\n",
      "*testing model:\n",
      "2821/2821 [==============================] - 1s 234us/step\n",
      "[0.973810351891156, 0.7242112755775452]\n"
     ]
    }
   ],
   "source": [
    "model=CNN_define(embedding_layer,EMBEDDING_DIM,MAX_SEQUENCE_LENGTH)\n",
    "model_train('CNN_word2vec.h5',model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5f502bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*define model lstm：\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 100, 400)          82493600  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 200)               480800    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 14)                2814      \n",
      "=================================================================\n",
      "Total params: 82,977,214\n",
      "Trainable params: 483,614\n",
      "Non-trainable params: 82,493,600\n",
      "_________________________________________________________________\n",
      "['loss', 'acc']\n",
      "Train on 9026 samples, validate on 2257 samples\n",
      "Epoch 1/2\n",
      "9026/9026 [==============================] - 20s 2ms/step - loss: 1.7624 - acc: 0.4388 - val_loss: 1.0194 - val_acc: 0.6690\n",
      "Epoch 2/2\n",
      "9026/9026 [==============================] - 19s 2ms/step - loss: 1.0390 - acc: 0.6744 - val_loss: 1.3534 - val_acc: 0.5729\n",
      "*testing model:\n",
      "2821/2821 [==============================] - 2s 873us/step\n",
      "[1.0420007732310899, 0.6979794502258301]\n"
     ]
    }
   ],
   "source": [
    "model=LSTM_define(embedding_layer,EMBEDDING_DIM,MAX_SEQUENCE_LENGTH)\n",
    "model_train('LSTM_word2vec.h5',model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b53e0b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfid(all_texts):\n",
    "    print ('*doc to var:')\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer   \n",
    "    count_v0= CountVectorizer();  \n",
    "    counts_all = count_v0.fit_transform(all_texts);\n",
    "    count_v1= CountVectorizer(vocabulary=count_v0.vocabulary_);  \n",
    "    counts_train = count_v1.fit_transform(train_texts);   \n",
    "    print (\"the shape of train is \"+repr(counts_train.shape))  \n",
    "    count_v2 = CountVectorizer(vocabulary=count_v0.vocabulary_);  \n",
    "    counts_test = count_v2.fit_transform(test_texts);  \n",
    "    print (\"the shape of test is \"+repr(counts_test.shape))\n",
    "\n",
    "    tfidftransformer = TfidfTransformer();    \n",
    "    train_data = tfidftransformer.fit(counts_train).transform(counts_train);\n",
    "    test_data = tfidftransformer.fit(counts_test).transform(counts_test); \n",
    "\n",
    "    x_train = train_data\n",
    "    y_train = train_labels\n",
    "    x_test = test_data\n",
    "    y_test = test_labels\n",
    "    return x_train,y_train,x_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6849f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*doc to var:\n",
      "the shape of train is (5590, 201033)\n",
      "the shape of test is (8514, 201033)\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train,x_test,y_test=tfid(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e59a858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*KNN:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 1, precision_score:0.8202959830866807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 2, precision_score:0.7389006342494715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 3, precision_score:0.7854122621564482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 4, precision_score:0.7834155508574113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 5, precision_score:0.7891707775428706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 6, precision_score:0.7867042518205309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 7, precision_score:0.7936340145642471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 8, precision_score:0.7931642001409444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 9, precision_score:0.7950434578341555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 10, precision_score:0.7938689217758985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 11, precision_score:0.7946910970166784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 12, precision_score:0.7932816537467701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 13, precision_score:0.7928118393234672\n",
      "K= 14, precision_score:0.7933991073525958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "print ('*KNN:')\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "\n",
    "for x in range(1,15):  \n",
    "    knnclf = KNeighborsClassifier(n_neighbors=x)\n",
    "    knnclf.fit(x_train,y_train)  \n",
    "    preds = knnclf.predict(x_test)\n",
    "    num = 0\n",
    "    preds = preds.tolist()\n",
    "    for i,pred in enumerate(preds):\n",
    "        if int(pred) == int(y_test[i]):\n",
    "            num += 1\n",
    "    print ('K= '+str(x)+', precision_score:' + str(float(num) / len(preds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9bba25e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*SVM：\n",
      "precision_score:0.9837209302325581\n"
     ]
    }
   ],
   "source": [
    "print ('*SVM：')\n",
    "from sklearn.svm import SVC   \n",
    "svclf = SVC(kernel = 'linear') \n",
    "svclf.fit(x_train,y_train)  \n",
    "preds = svclf.predict(x_test)\n",
    "num = 0\n",
    "preds = preds.tolist()\n",
    "for i,pred in enumerate(preds):\n",
    "    if int(pred) == int(y_test[i]):\n",
    "        num += 1\n",
    "print ('precision_score:' + str(float(num) / len(preds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d75c41e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Naive Bayes：\n",
      "precision_score:0.9012215175005873\n"
     ]
    }
   ],
   "source": [
    "print ('*Naive Bayes：')\n",
    "from sklearn.naive_bayes import MultinomialNB  \n",
    "from sklearn import metrics\n",
    "clf = MultinomialNB(alpha = 0.01)   \n",
    "clf.fit(x_train, y_train);  \n",
    "preds = clf.predict(x_test)\n",
    "num = 0\n",
    "preds = preds.tolist()\n",
    "for i,pred in enumerate(preds):\n",
    "    if int(pred) == int(y_test[i]):\n",
    "        num += 1\n",
    "print ('precision_score:' + str(float(num) / len(preds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8cf3cb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Logisticre Gression：\n",
      "precision_score:0.8998120742306789\n"
     ]
    }
   ],
   "source": [
    "print ('*Logisticre Gression：')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "log =  LogisticRegression(C=1e10,max_iter=1000) \n",
    "log.fit(x_train, y_train)\n",
    "preds = log.predict(x_test)\n",
    "num = 0\n",
    "preds = preds.tolist()\n",
    "for i,pred in enumerate(preds):\n",
    "    if int(pred) == int(y_test[i]):\n",
    "        num += 1\n",
    "print ('precision_score:' + str(float(num) / len(preds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7203a4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Random Forest Classifier：\n",
      "precision_score:0.8998120742306789\n"
     ]
    }
   ],
   "source": [
    "print ('*Random Forest Classifier：')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "rfc =  RandomForestClassifier(n_estimators=500,  max_features='sqrt',random_state=10)\n",
    "rfc.fit(x_train, y_train)\n",
    "preds = log.predict(x_test)\n",
    "num = 0\n",
    "preds = preds.tolist()\n",
    "for i,pred in enumerate(preds):\n",
    "    if int(pred) == int(y_test[i]):\n",
    "        num += 1\n",
    "print ('precision_score:' + str(float(num) / len(preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e15f4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Decision Tree Classifier：\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-cbf743df9af7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdtc\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m77\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdtc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'log' is not defined"
     ]
    }
   ],
   "source": [
    "print ('*Decision Tree Classifier：')\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "dtc =  DecisionTreeClassifier(random_state=77)\n",
    "dtc.fit(x_train, y_train)\n",
    "preds = log.predict(x_test)\n",
    "num = 0\n",
    "preds = preds.tolist()\n",
    "for i,pred in enumerate(preds):\n",
    "    if int(pred) == int(y_test[i]):\n",
    "        num += 1\n",
    "print ('precision_score:' + str(float(num) / len(preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737a5c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
