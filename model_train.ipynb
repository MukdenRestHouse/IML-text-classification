{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64ce7843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model involved in the training are as follows：\n",
    "# CNN, Wordvec+CNN\n",
    "# LSTM, word2vec+LSTM\n",
    "# KNN\n",
    "# SVM\n",
    "# naive bayes\n",
    "# Logistic Regression\n",
    "# Random forest classifier\n",
    "# Decition tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd71f0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras \n",
    "# !pip install numpy\n",
    "# !pip install gensim\n",
    "\n",
    "# print(np.__version__)\n",
    "# print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be7fc9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(train_texts,train_labels,test_texts,test_labels):\n",
    "    #coding:utf-8\n",
    "    print ('*load texts:')\n",
    "    train_texts = open(train_texts,encoding='UTF-8').read().split('\\n')\n",
    "    train_labels = open(train_labels,encoding='UTF-8').read().split('\\n')\n",
    "    test_texts = open(test_texts,encoding='UTF-8').read().split('\\n')\n",
    "    test_labels = open(test_labels,encoding='UTF-8').read().split('\\n')\n",
    "    all_texts = train_texts + test_texts\n",
    "    all_labels = train_labels + test_labels\n",
    "    return all_texts,all_labels,train_texts,train_labels,test_texts,test_labels\n",
    "    \n",
    "def creat_tokenizer(all_texts):\n",
    "    print ('*tokenizer:')\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    from tensorflow.keras.utils import to_categorical\n",
    "    import numpy as np\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(all_texts)\n",
    "    sequences = tokenizer.texts_to_sequences(all_texts)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    labels = to_categorical(np.asarray(all_labels))\n",
    "    print('The shape of data tensor:', data.shape)\n",
    "    print('The shape of label tensor:', labels.shape)\n",
    "    return data,labels,word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2be6e75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\scipy\\__init__.py:173: UserWarning: A NumPy version >=1.19.5 and <1.27.0 is required for this version of SciPy (detected version 1.19.1)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*load texts:\n",
      "*tokenizer:\n",
      "Found 205393 unique tokens.\n",
      "The shape of data tensor: (14104, 200)\n",
      "The shape of label tensor: (14104, 14)\n",
      "*split data :\n",
      "len of train : 9167\n",
      "len of val : 2116\n",
      "len of test : 2821\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras\n",
    "import keras\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "EMBEDDING_DIM = 400\n",
    "VALIDATION_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.2\n",
    "\n",
    "all_texts,all_labels,train_texts,train_labels,test_texts,test_labels=load_text('text_stopwords.txt','categroy.txt','new_text_stopwords.txt','ner_cate.txt')\n",
    "\n",
    "data,labels,word_index=creat_tokenizer(all_texts)\n",
    "\n",
    "print ('*split data :')\n",
    "# split the data into training set, validation set, and test set\n",
    "p1 = int(len(data)*(1-VALIDATION_SPLIT-TEST_SPLIT))\n",
    "p2 = int(len(data)*(1-TEST_SPLIT))\n",
    "x_train = data[:p1]\n",
    "y_train = labels[:p1]\n",
    "x_val = data[p1:p2]\n",
    "y_val = labels[p1:p2]\n",
    "x_test = data[p2:]\n",
    "y_test = labels[p2:]\n",
    "print ('len of train : '+str(len(x_train)))\n",
    "print ('len of val : '+str(len(x_val)))\n",
    "print ('len of test : '+str(len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e34b4978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_define(embedding_layer,embedding_dim,max_sequence_length):\n",
    "    #define for CNN\n",
    "    print ('*define model CNN:')\n",
    "    from keras.layers import Dense, Input, Flatten, Dropout\n",
    "    from keras.layers import Conv1D, MaxPooling1D, Embedding, GlobalMaxPooling1D\n",
    "    from keras.models import Sequential\n",
    "\n",
    "    model = Sequential()\n",
    "    if embedding_layer==None:  \n",
    "        model.add(Embedding(len(word_index) + 1, embedding_dim, input_length=max_sequence_length))\n",
    "    else:\n",
    "        model.add(embedding_layer)\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(300, 3, padding='valid', activation='relu', strides=1))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(embedding_dim, activation='relu'))\n",
    "    model.add(Dense(labels.shape[1], activation='softmax'))\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
    "    print (model.metrics_names)\n",
    "    return model\n",
    "\n",
    "def LSTM_define(embedding_layer,embedding_dim,max_sequence_length):\n",
    "    #trainning for lstm\n",
    "    print ('*define model lstm：')\n",
    "    from keras.layers import Dense, Input, Flatten, Dropout\n",
    "    from keras.layers import LSTM, Embedding\n",
    "    from keras.models import Sequential\n",
    "\n",
    "    model = Sequential()\n",
    "    if embedding_layer==None:\n",
    "        model.add(Embedding(len(word_index) + 1, embedding_dim, input_length=max_sequence_length))\n",
    "    else:\n",
    "        model.add(embedding_layer)\n",
    "    model.add(LSTM(200, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(labels.shape[1], activation='softmax'))\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
    "    print (model.metrics_names)\n",
    "    return model\n",
    "    \n",
    "def model_train(model_name,model):\n",
    "    model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=2, batch_size=128)\n",
    "    model.save(model_name)\n",
    "\n",
    "    print ('*testing model:')\n",
    "    print (model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e0fe03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*define model CNN:\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 400)          82157600  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200, 400)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 198, 300)          360300    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 66, 300)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 19800)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 400)               7920400   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 14)                5614      \n",
      "=================================================================\n",
      "Total params: 90,443,914\n",
      "Trainable params: 90,443,914\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "['loss', 'acc']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9167 samples, validate on 2116 samples\n",
      "Epoch 1/2\n",
      "9167/9167 [==============================] - 58s 6ms/step - loss: 1.9821 - acc: 0.3774 - val_loss: 1.5118 - val_acc: 0.4830\n",
      "Epoch 2/2\n",
      "9167/9167 [==============================] - 58s 6ms/step - loss: 0.7239 - acc: 0.7716 - val_loss: 0.5334 - val_acc: 0.8568\n",
      "*testing model:\n",
      "2821/2821 [==============================] - 1s 437us/step\n",
      "[0.9659175089342887, 0.7096773982048035]\n"
     ]
    }
   ],
   "source": [
    "model=CNN_define(None,EMBEDDING_DIM,MAX_SEQUENCE_LENGTH)\n",
    "model_train('CNN.h5',model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f15f7fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*define model lstm：\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 200, 400)          82157600  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               480800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 14)                2814      \n",
      "=================================================================\n",
      "Total params: 82,641,214\n",
      "Trainable params: 82,641,214\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "['loss', 'acc']\n",
      "Train on 9167 samples, validate on 2116 samples\n",
      "Epoch 1/2\n",
      "9167/9167 [==============================] - 93s 10ms/step - loss: 2.2174 - acc: 0.2672 - val_loss: 2.1782 - val_acc: 0.2183\n",
      "Epoch 2/2\n",
      "9167/9167 [==============================] - 262s 29ms/step - loss: 1.4141 - acc: 0.5650 - val_loss: 1.6114 - val_acc: 0.4920\n",
      "*testing model:\n",
      "2821/2821 [==============================] - 5s 2ms/step\n",
      "[1.5141871000426494, 0.5455512404441833]\n"
     ]
    }
   ],
   "source": [
    "model=LSTM_define(None,EMBEDDING_DIM,MAX_SEQUENCE_LENGTH)\n",
    "model_train('LSTM.h5',model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ade9c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4) load word2vec as embedding...\n",
      "59395 words not in w2v model\n"
     ]
    }
   ],
   "source": [
    "print ('(4) load word2vec as embedding...')\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('wiki.zh.text.vector', binary=False)\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "not_in_model = 0\n",
    "for word, i in word_index.items(): \n",
    "    if str(word) in w2v_model:\n",
    "        embedding_matrix[i] = np.asarray(w2v_model[str(word)], dtype='float32')\n",
    "    else:\n",
    "        not_in_model += 1\n",
    "print (str(not_in_model)+' words not in w2v model')\n",
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(len(word_index) + 1,EMBEDDING_DIM,weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62402289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*define model CNN:\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 200, 400)          82157600  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200, 400)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 198, 300)          360300    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 66, 300)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 19800)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 400)               7920400   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 14)                5614      \n",
      "=================================================================\n",
      "Total params: 90,443,914\n",
      "Trainable params: 8,286,314\n",
      "Non-trainable params: 82,157,600\n",
      "_________________________________________________________________\n",
      "['loss', 'acc']\n",
      "Train on 9167 samples, validate on 2116 samples\n",
      "Epoch 1/2\n",
      "9167/9167 [==============================] - 17s 2ms/step - loss: 7.7625 - acc: 0.4548 - val_loss: 0.7718 - val_acc: 0.7892\n",
      "Epoch 2/2\n",
      "9167/9167 [==============================] - 16s 2ms/step - loss: 0.8055 - acc: 0.7792 - val_loss: 0.4040 - val_acc: 0.8908\n",
      "*testing model:\n",
      "2821/2821 [==============================] - 1s 477us/step\n",
      "[1.143060540305685, 0.7259836792945862]\n"
     ]
    }
   ],
   "source": [
    "model=CNN_define(embedding_layer,EMBEDDING_DIM,MAX_SEQUENCE_LENGTH)\n",
    "model_train('CNN_word2vec.h5',model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b53baf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*define model lstm：\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 200, 400)          82157600  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 200)               480800    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 14)                2814      \n",
      "=================================================================\n",
      "Total params: 82,641,214\n",
      "Trainable params: 483,614\n",
      "Non-trainable params: 82,157,600\n",
      "_________________________________________________________________\n",
      "['loss', 'acc']\n",
      "Train on 9167 samples, validate on 2116 samples\n",
      "Epoch 1/2\n",
      "9167/9167 [==============================] - 40s 4ms/step - loss: 1.5012 - acc: 0.5380 - val_loss: 1.1338 - val_acc: 0.6971\n",
      "Epoch 2/2\n",
      "9167/9167 [==============================] - 39s 4ms/step - loss: 0.9390 - acc: 0.7072 - val_loss: 0.8816 - val_acc: 0.7250\n",
      "*testing model:\n",
      "2821/2821 [==============================] - 4s 2ms/step\n",
      "[0.8800862259536887, 0.7422899603843689]\n"
     ]
    }
   ],
   "source": [
    "model=LSTM_define(embedding_layer,EMBEDDING_DIM,MAX_SEQUENCE_LENGTH)\n",
    "model_train('LSTM_word2vec.h5',model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b53e0b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfid(all_texts):\n",
    "    print ('*tfidtransfor:')\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer   \n",
    "    count_v0= CountVectorizer()\n",
    "    counts_all = count_v0.fit_transform(all_texts)\n",
    "    \n",
    "    count_v1= CountVectorizer(vocabulary=count_v0.vocabulary_)\n",
    "    counts_train = count_v1.fit_transform(train_texts);   \n",
    "    print (\"the shape of train is \"+repr(counts_train.shape))  \n",
    "    count_v2 = CountVectorizer(vocabulary=count_v0.vocabulary_)\n",
    "    counts_test = count_v2.fit_transform(test_texts);  \n",
    "    print (\"the shape of test is \"+repr(counts_test.shape))\n",
    "\n",
    "    tfidftransformer = TfidfTransformer();    \n",
    "    train_data = tfidftransformer.fit(counts_train).transform(counts_train);\n",
    "    test_data = tfidftransformer.fit(counts_test).transform(counts_test); \n",
    "\n",
    "    x_train = train_data\n",
    "    y_train = train_labels\n",
    "    x_test = test_data\n",
    "    y_test = test_labels\n",
    "    return x_train,y_train,x_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6849f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*tfidtransfor:\n",
      "the shape of train is (5590, 200364)\n",
      "the shape of test is (8514, 200364)\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train,x_test,y_test=tfid(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bea3acb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model,x_test,y_test):\n",
    "    preds=model.predict(x_test)\n",
    "    num=0\n",
    "    preds=preds.tolist()\n",
    "    for i,pred in enumerate(preds):\n",
    "        if int(pred) == int(y_test[i]):\n",
    "            num += 1\n",
    "    return num, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e59a858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*KNN:\n",
      "K= 1, precision_score:0.8202959830866807\n",
      "f1_score:0.7174386211598571\n",
      "K= 2, precision_score:0.8202959830866807\n",
      "f1_score:0.7174386211598571\n",
      "K= 3, precision_score:0.8424947145877378\n",
      "f1_score:0.7423631408084461\n",
      "K= 4, precision_score:0.8489546629081512\n",
      "f1_score:0.7469903378019884\n",
      "K= 5, precision_score:0.8557669720460418\n",
      "f1_score:0.759879310359314\n",
      "K= 6, precision_score:0.8594080338266384\n",
      "f1_score:0.7655601346744287\n",
      "K= 7, precision_score:0.8630490956072352\n",
      "f1_score:0.7691829155578781\n",
      "K= 8, precision_score:0.8659854357528776\n",
      "f1_score:0.771919421156711\n",
      "K= 9, precision_score:0.8642236316654921\n",
      "f1_score:0.7727772443718158\n"
     ]
    }
   ],
   "source": [
    "print ('*KNN:')\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "for x in range(1,10):  \n",
    "    knnclf = KNeighborsClassifier(n_neighbors=x,weights='distance', algorithm='auto', leaf_size=30)\n",
    "    knnclf.fit(x_train,y_train)  \n",
    "    num, preds=test_model(knnclf,x_test,y_test)\n",
    "    print ('K= '+str(x)+', precision_score:' + str(float(num) / len(preds)))\n",
    "    print ('f1_score:' + str(f1_score(y_test,preds,average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9bba25e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*SVM：(rbf)\n",
      "precision_score:0.8925299506694856\n",
      "f1_score:0.8168375820073931\n",
      "*SVM：(linear)\n",
      "precision_score:0.9007517030772845\n",
      "f1_score:0.8149666268208976\n",
      "*SVM：(poly)\n",
      "precision_score:0.7077754287056612\n",
      "f1_score:0.64867385708012\n"
     ]
    }
   ],
   "source": [
    "print ('*SVM：(rbf)')\n",
    "from sklearn.svm import SVC   \n",
    "svclf_rbf = SVC(kernel = 'rbf') \n",
    "svclf_rbf.fit(x_train,y_train)  \n",
    "num, preds=test_model(svclf_rbf,x_test,y_test)\n",
    "print ('precision_score:' + str(float(num) / len(preds)))\n",
    "print ('f1_score:' + str(f1_score(y_test,preds,average='macro')))\n",
    "\n",
    "print ('*SVM：(linear)')\n",
    "svclf_lin = SVC(kernel = 'linear') \n",
    "svclf_lin.fit(x_train,y_train)  \n",
    "num, preds=test_model(svclf_lin,x_test,y_test)\n",
    "print ('precision_score:' + str(float(num) / len(preds)))\n",
    "print ('f1_score:' + str(f1_score(y_test,preds,average='macro')))\n",
    "\n",
    "print ('*SVM：(poly)')\n",
    "from sklearn.svm import SVC   \n",
    "svclf_poly = SVC(kernel = 'poly')\n",
    "svclf_poly.fit(x_train,y_train)  \n",
    "num, preds=test_model(svclf_poly,x_test,y_test)\n",
    "print ('precision_score:' + str(float(num) / len(preds)))\n",
    "print ('f1_score:' + str(f1_score(y_test,preds,average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d75c41e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Naive Bayes：\n",
      "precision_score:0.9021611463471929\n",
      "f1_score:0.8212736580324363\n"
     ]
    }
   ],
   "source": [
    "print ('*Naive Bayes：')\n",
    "from sklearn.naive_bayes import MultinomialNB  \n",
    "\n",
    "clf = MultinomialNB(alpha = 0.01)   \n",
    "clf.fit(x_train, y_train)\n",
    "num, preds=test_model(clf,x_test,y_test)\n",
    "print ('precision_score:' + str(float(num) / len(preds)))\n",
    "print ('f1_score:' + str(f1_score(y_test,preds,average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cf3cb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Logistic Regression：\n",
      "precision_score:0.9003993422598073\n",
      "f1_score:0.8039468213952466\n"
     ]
    }
   ],
   "source": [
    "print ('*Logistic Regression：')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log =  LogisticRegression(C=1e10,max_iter=1000) \n",
    "log.fit(x_train, y_train)\n",
    "num, preds=test_model(log,x_test,y_test)\n",
    "print ('precision_score:' + str(float(num) / len(preds)))\n",
    "print ('f1_score:' + str(f1_score(y_test,preds,average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7203a4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Random Forest Classifier：\n",
      "precision_score:0.8803147756636128\n",
      "f1_score:0.814149079603147\n"
     ]
    }
   ],
   "source": [
    "print ('*Random Forest Classifier：')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc =  RandomForestClassifier(n_estimators=500,  max_features='sqrt',random_state=10)\n",
    "rfc.fit(x_train, y_train)\n",
    "num, preds=test_model(rfc,x_test,y_test)\n",
    "print ('precision_score:' + str(float(num) / len(preds)))\n",
    "print ('f1_score:' + str(f1_score(y_test,preds,average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3dda5080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Decision Tree Classifier: \n",
      "precision_score:0.7653276955602537\n",
      "f1_score:0.6698640781213243\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# depth_set=[3,4,5]\n",
    "# for i in depth_set:\n",
    "print ('*Decision Tree Classifier: ')\n",
    "dtc =  DecisionTreeClassifier(random_state=0, splitter='best')\n",
    "dtc.fit(x_train, y_train)\n",
    "num, preds=test_model(dtc,x_test,y_test)\n",
    "print ('precision_score:' + str(float(num) / len(preds)))\n",
    "print ('f1_score:' + str(f1_score(y_test,preds,average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed554fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
